<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="date" content="2018-03-10" />

<title>GeoConvention 2018   May 7-9, Calgary, Canada   Title here</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.6/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.6/js/bootstrap.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<script src="libs/navigation-1.1/codefolding.js"></script>
<link href="libs/magnific-popup-1.1.0/magnific-popup.css" rel="stylesheet" />
<script src="libs/magnific-popup-1.1.0/jquery.magnific-popup.min.js"></script>
<link href="libs/readthedown-0.1/readthedown.css" rel="stylesheet" />
<script src="libs/readthedown-0.1/readthedown.js"></script>



<link rel="stylesheet" href="custom.css" type="text/css" />

</head>

<body>


<div id="content" data-toggle="wy-nav-shift">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->

<nav id="nav-top" role="navigation" aria-label="top navigation">
    <a role="button" href="#" data-toggle="wy-nav-top"><span class="glyphicon glyphicon-menu-hamburger"></span></a>
</nav>


<div id="header">
<h1 class="title">GeoConvention 2018 <br> May 7-9, Calgary, Canada <br> Title here</h1>
</div>


<div id="table-of-contents">
    <h2><a href="#content">GeoConvention 2018 <br> May 7-9, Calgary, Canada <br> Title here</a></h2>
    <div id="text-table-of-contents">
      <ul>
      <li><a href="#is-there-a-difference-in-production-the-kruskal-wallis-test">Is There A Difference in Production? The Kruskal-Wallis Test</a></li>
      <li><a href="#distance-correlation">Distance Correlation</a></li>
      <li><a href="#smoothers">Smoothers</a></li>
      <li><a href="#variable-selection">Variable Selection</a><ul>
      <li><a href="#clustering">Clustering</a></li>
      <li><a href="#redundancy-analysis">Redundancy Analysis</a></li>
      <li><a href="#lasso">LASSO</a></li>
      <li><a href="#recursive-feature-elimination">Recursive feature elimination</a></li>
      <li><a href="#summary-of-variable-selection">Summary of Variable Selection</a></li>
      </ul></li>
      <li><a href="#exhaustive-search">Exhaustive Search</a></li>
      <li><a href="#outliers-and-extremes">Outliers and Extremes</a></li>
      </ul>
    </div>
</div>

<div id="main">
<table class="table table-striped table-hover table-condensed" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
gross.pay
</th>
<th style="text-align:left;">
phi.h
</th>
<th style="text-align:left;">
position
</th>
<th style="text-align:left;">
pressure
</th>
<th style="text-align:left;">
random.1
</th>
<th style="text-align:left;">
random.2
</th>
<th style="text-align:left;">
gross.pay.transform
</th>
<th style="text-align:left;">
production
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.5
</td>
<td style="text-align:left;">
2.1
</td>
<td style="text-align:left;">
19
</td>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
379
</td>
<td style="text-align:left;">
3.54
</td>
<td style="text-align:left;">
15.10
</td>
</tr>
<tr>
<td style="text-align:left;">
1.0
</td>
<td style="text-align:left;">
4.0
</td>
<td style="text-align:left;">
1.1
</td>
<td style="text-align:left;">
16
</td>
<td style="text-align:left;">
13
</td>
<td style="text-align:left;">
269
</td>
<td style="text-align:left;">
5.79
</td>
<td style="text-align:left;">
21.30
</td>
</tr>
<tr>
<td style="text-align:left;">
1.9
</td>
<td style="text-align:left;">
19.0
</td>
<td style="text-align:left;">
1.0
</td>
<td style="text-align:left;">
14
</td>
<td style="text-align:left;">
12
</td>
<td style="text-align:left;">
245
</td>
<td style="text-align:left;">
8.51
</td>
<td style="text-align:left;">
22.75
</td>
</tr>
<tr>
<td style="text-align:left;">
3.1
</td>
<td style="text-align:left;">
21.7
</td>
<td style="text-align:left;">
2.1
</td>
<td style="text-align:left;">
17
</td>
<td style="text-align:left;">
6
</td>
<td style="text-align:left;">
273
</td>
<td style="text-align:left;">
11.52
</td>
<td style="text-align:left;">
15.72
</td>
</tr>
<tr>
<td style="text-align:left;">
4.1
</td>
<td style="text-align:left;">
24.6
</td>
<td style="text-align:left;">
2.9
</td>
<td style="text-align:left;">
11
</td>
<td style="text-align:left;">
10
</td>
<td style="text-align:left;">
237
</td>
<td style="text-align:left;">
10.16
</td>
<td style="text-align:left;">
7.71
</td>
</tr>
</tbody>
</table>
<div id="is-there-a-difference-in-production-the-kruskal-wallis-test" class="section level1">
<h1>Is There A Difference in Production? The Kruskal-Wallis Test</h1>
<p>Suppose we are interested in assessing whether <code>production</code> changes according to <code>position</code>. The conventional approach is to perform ANOVA. However, ANOVA relies on normality and is sensitive to outliers. We can utilize the Kruskal-Wallis test, which is a rank based (i.e. non-parametric) test.</p>
<p>From the boxplot below, it is not apparent whether there is a difference in production between <code>positions</code>:</p>
<p><img src="Figures/unnamed-chunk-4-1.svg" width="500px" /></p>
<p><br></p>
<p>Let’s perform the <strong>Kruskal-Wallis</strong> test:</p>
<table class="table table-striped table-hover table-condensed" style="width: auto !important; ">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
d.f.
</th>
<th style="text-align:left;">
Partial.SS
</th>
<th style="text-align:left;">
MS
</th>
<th style="text-align:left;">
F
</th>
<th style="text-align:left;">
P
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;">
position.cat
</td>
<td style="text-align:left;font-weight: bold;">
1
</td>
<td style="text-align:left;font-weight: bold;">
163.528
</td>
<td style="text-align:left;font-weight: bold;">
163.528
</td>
<td style="text-align:left;font-weight: bold;">
5.123
</td>
<td style="text-align:left;font-weight: bold;">
0.036
</td>
</tr>
<tr>
<td style="text-align:left;">
TOTAL
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
163.528
</td>
<td style="text-align:left;">
163.528
</td>
<td style="text-align:left;">
5.123
</td>
<td style="text-align:left;">
0.036
</td>
</tr>
<tr>
<td style="text-align:left;">
ERROR
</td>
<td style="text-align:left;">
19
</td>
<td style="text-align:left;">
606.472
</td>
<td style="text-align:left;">
31.920
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:left;">
NA
</td>
</tr>
</tbody>
</table>
<p>A P-value of 0.036, tells us that the probability of observing a difference in distribution equal to or more extreme than the one observed is 3.6%. In other words, <strong>it’s rather unlikely that the difference in production is due to chance variation</strong>.</p>
<p><br></p>
</div>
<div id="distance-correlation" class="section level1">
<h1>Distance Correlation</h1>
<p>The traditional Pearson correlation has numerous limitations. The Spearman correlation is generally preferred and avoids some of the limitations. A relatively new and powerful measure of correlation is the <strong><a href="https://en.wikipedia.org/wiki/Distance_correlation">distance correlation</a></strong> [1].</p>
<div class="figure">
<img src="Figures/unnamed-chunk-6-1.svg" alt="Distance correlation matrix. Colored boxes represent statistically significant correlations at the 10% significance level." />
<p class="caption">Distance correlation matrix. Colored boxes represent statistically significant correlations at the 10% significance level.</p>
</div>
<p>[1] <em>Among measures of correlations capable of detecting non-linear association, MIC (maximal information criterion, Reshef et al. (2011)) has been touted as a powerful measures. However, several authors (Kinney and Atwal (2013), Simon and Tibshirani (2014)) point to a number of problems with MIC and suggest to utilize distance correlation instead: “We believe that the recently proposed distance correlation measure of Székely &amp; Rizzo (2009) is a more powerful technique that is simple, easy to compute and should be considered for general use”.</em></p>
<p><br><br></p>
</div>
<div id="smoothers" class="section level1">
<h1>Smoothers</h1>
<p>Correlation matrices are useful to visualize combinations of pairwise correlations. There is, however, more information to be had by plotting the data and adding a trend line. One of the most flexible is Cleveland’s <a href="https://en.wikipedia.org/wiki/Local_regression">LOESS</a> (locally weighted scatterplot smoothing). Let’s plot the LOESS curves for each variable against production, separate for each <code>position</code>.</p>
<p><img src="Figures/unnamed-chunk-7-1.svg" /><!-- --></p>
<p><em>Scatterplots of production against each variable by position (red: blue:). A LOESS curve was added to highlight the overall trend.</em></p>
<p><br><br></p>
</div>
<div id="variable-selection" class="section level1">
<h1>Variable Selection</h1>
<p>Which variables should we focus on in understanding production? Which variables are measuring the same or similar underlying quantities? As the sample size decreases, a model will likely overfit the data. It then becomes important to start with our analysis and modeling with fewer variables if possible than the ones present int he data.</p>
<p>A statistically principled approach is to start by eliminating variables based on domain knowledge. This may be assisted by statistical data reduction methods such as clustering and redundancy analysis.</p>
<p><br></p>
<div id="clustering" class="section level2">
<h2>Clustering</h2>
<p>Here we assess clusters of independent variables.</p>
<p><img src="Figures/unnamed-chunk-8-1.svg" /><!-- --></p>
<p><em>Hierarchical cluster dendogram of all variables except production. The similarity matrix is based on the Hoeffding D statistics which will detect non-monotonic associations.</em></p>
<p>There are numerous methods to perform hierarchical clustering. It’s always good practice to try different methods to see of the results are in the same ballpark. Let’s do that with Hierarchical Clustering with P-Values via Multiscale Bootstrap:</p>
<pre><code>## Creating a temporary cluster...done:
## socket cluster with 7 nodes on host &#39;localhost&#39;
## Multiscale bootstrap... Done.</code></pre>
<p><img src="Figures/unnamed-chunk-9-1.svg" /><!-- --></p>
<p>From either cluster dendograms we can see how <code>gross.pay</code> and <code>gross.pay.transform</code> cluster together and, to a lesser extent, <code>phi.h</code>.</p>
<p><br></p>
</div>
<div id="redundancy-analysis" class="section level2">
<h2>Redundancy Analysis</h2>
<p>In redundancy analysis we look at which variable can be predicted with high confidence from any combination of the other variables. Those variables can then be safely omitted from further analysis. This approach is more robust than the pairwise correlation measures from before.</p>
<pre><code>## 
## Redundancy Analysis
## 
## redun(formula = ~gross.pay + phi.h + I(position.cat) + pressure + 
##     random.1 + random.2 + gross.pay.transform, data = hunt, r2 = 0.8, 
##     type = &quot;adjusted&quot;, tlinear = FALSE, iterms = TRUE, pc = TRUE)
## 
## n: 21    p: 13   nk: 3 
## 
## Number of NAs:    0 
## 
## R-squared cutoff: 0.8    Type: adjusted 
## 
## R^2 with which each variable can be predicted from all other variables:
## 
##            gross.pay           gross.pay&#39;                phi.h 
##                0.973                0.889                0.864 
##               phi.h&#39;      I(position.cat)             pressure 
##                0.815                0.478                0.000 
##            pressure&#39;             random.1            random.1&#39; 
##                0.689                0.000                0.676 
##             random.2            random.2&#39;  gross.pay.transform 
##                0.000                0.281                0.952 
## gross.pay.transform&#39; 
##                0.878 
## 
## Rendundant variables:
## 
## gross.pay gross.pay.transform&#39;
## 
## Predicted from variables:
## 
## gross.pay&#39; phi.h phi.h&#39; I(position.cat) pressure pressure&#39; random.1 random.1&#39; random.2 random.2&#39; gross.pay.transform 
## 
##       Variable Deleted   R^2 R^2 after later deletions
## 1            gross.pay 0.973                     0.976
## 2 gross.pay.transform&#39; 0.892</code></pre>
<p>We’ve set the adjusted R<sup>2</sup> value at 0.80. For instance, <code>gross.pay</code> and <code>gross.pay.transform</code> are redundant because they can be predicted with high confidence from all other predictors.</p>
<p><br></p>
</div>
<div id="lasso" class="section level2">
<h2>LASSO</h2>
<p>The previous methods achieved variable reduction without consideration of the response variable, <code>production</code>. This is a sound approach, however, we may wish to do variable selection within a regression model. LASSO (least absolute shrinkage and selection operator) achieves that by shrinking the model coefficients. If some coefficients are shrinked to zero, the LASSO effectively achieves variable selection. Shrinkage methods like the LASSO are useful to improve the performance of a model and control overfitting.</p>
<p>There are several models besides the LASSO that allow built-in variable selection.</p>
<pre><code>## 8 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                                    1
## (Intercept)              -4.27844527
## hunt.gross.pay            1.03153068
## hunt.phi.h                0.11603225
## hunt.pressure             1.33163584
## hunt.random.1             .         
## hunt.random.2             0.01013075
## hunt.gross.pay.transform  .         
## position.cat             -6.54390827</code></pre>
<p><img src="Figures/unnamed-chunk-11-1.svg" /><!-- --></p>
<p>Looking at the table of coefficients, notice how <code>hunt.random.1</code> and <code>gross.pay.transform</code> have no coefficient. That’s because it has been shrunk to zero, thus achieving variable selection. Our model suggests that these two variable are not useful in explaining changes in <code>production</code>. The coefficient for <code>hunt.random.2</code> is also nearly zero, so we could remove it as well.</p>
<p><br></p>
</div>
<div id="recursive-feature-elimination" class="section level2">
<h2>Recursive feature elimination</h2>
<p>The last method we want to illustrate is that of recursive feature elimination. This is a model-based set of methods.</p>
<table class="table table-striped table-hover table-condensed" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Variables
</th>
<th style="text-align:left;">
RMSE
</th>
<th style="text-align:left;">
Rsquared
</th>
<th style="text-align:left;">
MAE
</th>
<th style="text-align:left;">
RMSESD
</th>
<th style="text-align:left;">
RsquaredSD
</th>
<th style="text-align:left;">
MAESD
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
8.961
</td>
<td style="text-align:left;">
0.971
</td>
<td style="text-align:left;">
8.065
</td>
<td style="text-align:left;">
4.740
</td>
<td style="text-align:left;">
0.124
</td>
<td style="text-align:left;">
4.115
</td>
</tr>
<tr>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
7.391
</td>
<td style="text-align:left;">
0.981
</td>
<td style="text-align:left;">
6.633
</td>
<td style="text-align:left;">
3.952
</td>
<td style="text-align:left;">
0.100
</td>
<td style="text-align:left;">
3.559
</td>
</tr>
<tr>
<td style="text-align:left;">
3
</td>
<td style="text-align:left;">
6.079
</td>
<td style="text-align:left;">
0.986
</td>
<td style="text-align:left;">
5.525
</td>
<td style="text-align:left;">
2.529
</td>
<td style="text-align:left;">
0.071
</td>
<td style="text-align:left;">
2.472
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;color: black;background-color: #FFFED0;">
4
</td>
<td style="text-align:left;font-weight: bold;color: black;background-color: #FFFED0;">
3.428
</td>
<td style="text-align:left;font-weight: bold;color: black;background-color: #FFFED0;">
0.994
</td>
<td style="text-align:left;font-weight: bold;color: black;background-color: #FFFED0;">
3.034
</td>
<td style="text-align:left;font-weight: bold;color: black;background-color: #FFFED0;">
1.721
</td>
<td style="text-align:left;font-weight: bold;color: black;background-color: #FFFED0;">
0.037
</td>
<td style="text-align:left;font-weight: bold;color: black;background-color: #FFFED0;">
1.569
</td>
</tr>
<tr>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
3.610
</td>
<td style="text-align:left;">
0.992
</td>
<td style="text-align:left;">
3.209
</td>
<td style="text-align:left;">
1.678
</td>
<td style="text-align:left;">
0.057
</td>
<td style="text-align:left;">
1.526
</td>
</tr>
<tr>
<td style="text-align:left;">
6
</td>
<td style="text-align:left;">
3.570
</td>
<td style="text-align:left;">
0.989
</td>
<td style="text-align:left;">
3.187
</td>
<td style="text-align:left;">
1.803
</td>
<td style="text-align:left;">
0.070
</td>
<td style="text-align:left;">
1.630
</td>
</tr>
<tr>
<td style="text-align:left;">
7
</td>
<td style="text-align:left;">
3.581
</td>
<td style="text-align:left;">
0.989
</td>
<td style="text-align:left;">
3.162
</td>
<td style="text-align:left;">
1.862
</td>
<td style="text-align:left;">
0.068
</td>
<td style="text-align:left;">
1.690
</td>
</tr>
</tbody>
</table>
<p>From the table we can see a model with 4 variables achieves the highest R<sup>2</sup> and the lowest RMSE. Those 4 variables are:</p>
<pre><code>## [1] &quot;hunt.phi.h&quot;     &quot;hunt.gross.pay&quot; &quot;hunt.pressure&quot;  &quot;position.cat&quot;</code></pre>
<p>We can, of course, plot the results for more immediacy:</p>
<p><img src="Figures/unnamed-chunk-14-1.svg" /><!-- --></p>
<p><br></p>
</div>
<div id="summary-of-variable-selection" class="section level2">
<h2>Summary of Variable Selection</h2>
<p>There is little consensus among researchers on what constitutes a sensible variable selection strategy. In fact, many authors are critical of several variable selection techniques and suggest utilizing domain knowledge coupled with variable selection blinded to Y (see Harrell, 2015).</p>
<p>Our example is hardly demonstrative, since we are dealing with a very limited sample size. Nonetheless, we can summarize the results as follows:</p>
<table>
<colgroup>
<col width="16%" />
<col width="14%" />
<col width="40%" />
<col width="29%" />
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>No. of Variables Selected</th>
<th>Variable Removed</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Domain knowledge</td>
<td>4</td>
<td><code>random.1</code>, <code>random.2</code>, <code>gross.pay.transform</code></td>
<td>Baseline</td>
</tr>
<tr class="even">
<td>Clustering</td>
<td>NA</td>
<td><code>gross.pay</code> and <code>gross.pay.transform</code> can be represented by a single variable</td>
<td>Only suggestive, not a true variable selection technique</td>
</tr>
<tr class="odd">
<td>Redundancy Analysis</td>
<td>5</td>
<td><code>gross.pay</code>, <code>gross.pay.transform</code></td>
<td></td>
</tr>
<tr class="even">
<td>LASSO</td>
<td>5</td>
<td><code>random.1</code>, <code>gross.pay.transform</code></td>
<td><code>random.2</code> was very close to zero</td>
</tr>
<tr class="odd">
<td>Recursive Feature Elimination</td>
<td>4</td>
<td><code>random.1</code>, <code>random.2</code>, <code>gross.pay.transform</code></td>
<td></td>
</tr>
</tbody>
</table>
<p>From these limited data, recursive feature elimination performed best. There are many other variable selection strategies and we’ve only scratched the surface. Using domain knowledge <strong>before</strong> applying any statistical methods is usually preferred.</p>
<p><br></p>
</div>
</div>
<div id="exhaustive-search" class="section level1">
<h1>Exhaustive Search</h1>
<p>Another option to consider is that of performing an exhaustive search throughout the model space. Exhaustive search methods can be computationally taxing as “with more than a thousand models when p = 10 and a million when p = 20”. <a href="https://www.jstatsoft.org/article/view/v083i09">Tarr et al.</a> (2018) illustrate an implementation of exhaustive search through many bootstrap resamples: “if there exists a “correct” model of a particular model size it will be selected overwhelmingly more often than other models of the same size”.</p>
<p>This approach is intended the assist the analyst with the choice of model, rather than providing an answer.</p>
<p><img src="Figures/unnamed-chunk-15-1.svg" /><!-- --></p>
<p>The plots above illustrate what happens to the model fit measure (log-likelihood) when we remove a variable (higher is better). So for instance, in the upper left plot, there’s a clear separation when we remove <code>random.1</code>, so that resamples that do not contain <code>random.1</code> perform better than model that do contain <code>random.1</code>.</p>
<p>Finaly, we can investigate how variables are included as the penalty increases. In the plot <code>RV</code> is a random variable that can be used as reference.</p>
<p><img src="Figures/unnamed-chunk-17-1.svg" /><!-- --></p>
<p>As the amount of penalty increases, it become clearer what variables remain: <code>lposition.cat</code> is the strongest predictor no matter the penalty. The three random variables and <code>gross.pay.transform</code> are the fastest to fall out from bootstrap resamples.</p>
</div>
<div id="outliers-and-extremes" class="section level1">
<h1>Outliers and Extremes</h1>
</div>
</div>


</div>

<div id="postamble" data-toggle="wy-nav-shift" class="status">
<p class="date"><span class="glyphicon glyphicon-calendar"></span> 2018-03-10</p>
</div>


<script>
$(document).ready(function () {
 	  });
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
